---
title: "Predicting Music Emotion: Insights from Canadian Weather and Spotify Audio Features"
subtitle: "Danceability Dominates, while Temperature Plays a Minor Role in Shaping Positivity in the Top 50 Charts"
author: Cristina Su Lam
thanks: "Code and data are available at: https://github.com/cristinaasu/Spotify_Analysis"
date: today
date-format: long
abstract: "This study explores how musical features and temperature shape emotional perceptions of music, measured by valence—a scale capturing a track's positivity. Using Spotify’s top 50 Canadian charts and population-weighted mean temperatures from six major cities, we applied a multiple linear regression model. Danceability emerged as the strongest predictor of valence, with an effect size of 1.260, while scaled temperature had a smaller but significant effect of 0.007. These findings emphasize the interplay between intrinsic audio features and environmental contexts in influencing listener sentiment, providing practical applications for personalized playlists, mood-based marketing, and further research on cultural and environmental influences."
toc: true
fig_caption: yes
number-sections: true
format: pdf
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false
library(tidyverse)
library(ggplot2)
library(hexbin)
library(arrow)
library(kableExtra)
library(knitr)
library(modelsummary)
library(broom)
library(car)
```

```{r}
#| include: false
#| warning: false
#| message: false
# Load dataset 
analysis_data <- read_parquet("../data/02-analysis_data/analysis_data.parquet")
mlm_model <- readRDS("../models/mlm_model.rds")
```

# Introduction {#sec-intro}

Music is a deeply embedded and transformative part of people’s lives, providing experiences that range from solitary headphone listening on a chilly day to attending global live events. It fosters empathy, social bonding, and cultural understanding, connecting individuals across cultural and linguistic differences [@CLARKE201561]. A UK study by @anglada2023music revealed that musical preferences are influenced by weather; warmer temperatures encourage songs with high intensity and positive emotions, while increased rainfall reduces their popularity. This underscores how weather significantly shapes the musical landscape.

The digital age has provided unmatched access to music data, with platforms like Spotify offering rich datasets on song characteristics [@spotifyr]. Inspired by the previous research, this study aims to bridge the gap by examining the interplay of musical and environmental factors on listener emotions within the Canadian context. Using valence—a metric measuring emotions from 0 (negative or sad) to 1 (positive or happy)—the study evaluates how these elements collectively shape listener experiences.

Valence, the estimand of interest in this study, represents the true effect this research seeks to understand. While it itself is unobservable, this study estimates it through a structured analytical approach, ensuring that the methodology remains sharply focused on the central research question. As an abstract quantity, it captures the theoretical relationship independent of sample-specific variations, allowing the study to delineate true effects from noise and systematic biases inherent in observational data.

Using a multiple linear regression framework, this study analyzed data from Spotify’s Top 50 Canadian charts, combined with temperature data from six major cities, spanning June to November 2024. Danceability emerged as the strongest predictor of valence, underscoring its dominant influence on listener emotions. Artist-specific effects, such as the positive contributions of Brenda Lee's tracks and the negative impact of Future's music, were also notable. Interestingly, temperature—despite being the primary variable of interest—had a relatively minor influence, suggesting that weather exerts a limited direct effect on valence. These findings reinforce the need to integrate contextual and musical factors to design personalized recommendations and mood-based marketing strategies that resonate with diverse listener preferences.

The remainder of this paper is structured as follows. @sec-data describes the dataset and measurement process, while @sec-model outlines the methodology and model validation. @sec-results presents the results, highlighting the predictors of valence and their implications. Finally, @sec-discussion discusses the findings, limitations, and potential avenues for future research.

# Data {#sec-data}

## Overview {#sec-overview}

To analyze the relationship between song characteristics, temperature, and valence--data from Spotify and weather records from Canada's most populous cities were integrated. Specifically, the dataset included daily Top 50 songs in Canada from Spotify Charts [@spotify_charts], covering the period from June 1, 2024, to November 16, 2024. Using the Spotify API [@spotifyr], features such as Valence, Danceability, Acousticness, and Tempo were retrieved. Concurrently, weather data was obtained from @historical_weather_gc for six major cities: Toronto, Montreal, Edmonton, Calgary, Ottawa, and Vancouver.

Data cleaning was conducted in R [@citeR] using the tidyverse [@thereferencecanbewhatever] and arrow [@arrow] packages. The cities were selected for their large populations, and a new numerical variable, Mean Temp, was calculated by weighting each city's mean daily temperature by its population density and dividing by the total population density. This weighting was applied to account for the varying influence of each city based on its population size, ensuring that the derived temperature better reflects the overall climate experienced by the majority of the population. The final dataset consists of 8,450 observations and includes key variables such as Valence, Danceability, Acousticness, Tempo, Temperature, and Artist. The dataset was thoroughly checked for missing or incomplete values, confirming that all entries were complete and suitable for analysis. A sample of the cleaned dataset is displayed in [@tbl-spotifysongs].

For visualization and analysis, the knitr [@knitr], ggplot2 [@ggplot2], and kableExtra [@kableExtra] packages were utilized to generate professional tables and visualizations, effectively enhancing the presentation of results.

```{r}
#| label: tbl-spotifysongs
#| tbl-cap: Sample of Song's Characteristics
#| echo: false
#| warning: false
#| fig-align: center
analysis_data |> 
  select(Artist, Song, Valence, `Mean Temp`, Danceability, Acousticness, Tempo, Date) |> 
  mutate(
    `Mean Temp` = round(`Mean Temp`, 2),
    Acousticness = round(Acousticness, 3),
    Date = format(as.Date(Date), "%b %d") 
  ) |> 
  head(5) |> 
  kable(
    col.names = c("Artist", "Song", "Valence", "Mean Temp", "Dance", "Acoustic", "Tempo", "Date"),
  ) |> 
  kable_styling(
    full_width = FALSE, 
    position = "center", 
    font_size = 10
  ) |> 
  column_spec(1, width = "2.5cm") |>  
  column_spec(2, width = "2.7cm") |> 
  column_spec(3:8, width = "1.25cm") 
```

[@tbl-summary] provides summary statistics of the numerical variables, highlighting key patterns in the dataset. Valence, a measure of emotional positivity ranging from 0 to 1, spans 0.036 to 0.981, with a mean of 0.532, indicating a generally moderate mood across tracks. Mean Temperature, calculated as a population-weighted average for major Canadian cities, averages 16.9°C and ranges from 3.5°C to 24.8°C, reflecting climatic variation during the study period.

Danceability and Acousticness, also on a 0 to 1 scale, reveal contrasting trends. Danceability ranges from 0.234 to 0.943, with a mean of 0.632, suggesting most tracks are moderately to highly danceable. In contrast, Acousticness ranges from 0.000 to 0.968, with a mean of 0.228, indicating that tracks with electronic or synthetic instrumentation are more prevalent.

Tempo, measured in beats per minute (BPM), averages 126 BPM, with a wide range from 48.7 BPM for slower tracks to 203.8 BPM for faster ones. This diversity in tempo reflects a broad mix of musical styles, providing a strong foundation for examining their relationship with valence.

```{r}
#| label: tbl-summary
#| tbl-cap: Summary Statistics
#| echo: false
#| warning: false
#| fig-align: center
summary_table <- analysis_data %>%
  reframe(
    Variable = c("Valence", "Mean Temp", "Danceability", "Acousticness", "Tempo"),
    Mean = c(
      mean(Valence, na.rm = TRUE),
      mean(`Mean Temp`, na.rm = TRUE),
      mean(Danceability, na.rm = TRUE),
      mean(Acousticness, na.rm = TRUE),
      mean(Tempo, na.rm = TRUE)
    ),
    SD = c(
      sd(Valence, na.rm = TRUE),
      sd(`Mean Temp`, na.rm = TRUE),
      sd(Danceability, na.rm = TRUE),
      sd(Acousticness, na.rm = TRUE),
      sd(Tempo, na.rm = TRUE)
    ),
    Min = c(
      min(Valence, na.rm = TRUE),
      min(`Mean Temp`, na.rm = TRUE),
      min(Danceability, na.rm = TRUE),
      min(Acousticness, na.rm = TRUE),
      min(Tempo, na.rm = TRUE)
    ),
    Max = c(
      max(Valence, na.rm = TRUE),
      max(`Mean Temp`, na.rm = TRUE),
      max(Danceability, na.rm = TRUE),
      max(Acousticness, na.rm = TRUE),
      max(Tempo, na.rm = TRUE)
    )
  ) %>%
  mutate(
    Mean = round(Mean, 3),
    SD = round(SD, 3),
    Min = round(Min, 3),
    Max = round(Max, 3)
  )
kable(summary_table, format = "pipe")
```

## Measurement

As described in @sec-overview, weather data was sourced from @historical_weather_gc, which collects meteorological observations via a network of weather stations across Canada. These stations record variables such as temperature, precipitation, and wind speed, employing standardized instruments and methodologies. Data is captured hourly or daily, depending on the station, and is validated through automated quality control processes and manual verification. This rigorous approach ensures reliable and precise measurements that meet international meteorological standards. In the context of this analysis, one weather station per city was selected based on relevance and proximity; additional details on this selection process are provided in @sec-appendix-a.

Similarly, the music data originates from Spotify Charts [@spotify_charts], which rank the most-streamed songs in a given region daily. These rankings are compiled by aggregating anonymized user streaming data across its platform. Streams are verified to ensure authenticity, eliminating artificial plays generated by bots or fraudulent activity. Additionally, Spotify employs machine learning models to analyze the audio features of each song, such as Valence, Danceability, Acousticness, and Tempo. These features are derived from the acoustic and digital signal processing of tracks, providing a quantitative representation of each song's characteristics.

It is important to note that in this dataset, daily temperature values are repeated across all fifty songs for a given day, as they represent an average measurement for the day rather than song-specific conditions. Similarly, artist entries recur within the dataset, reflecting the daily rankings of the Top 50 streamed songs, which often include repeated tracks for artists with sustained popularity or multiple charting songs.

The combination of these trusted data sources ensures that the dataset used in this study is grounded in accurate and validated measurements. Weather data reflects real-world environmental conditions, while Spotify's data captures both user engagement and precise audio analysis, enabling a comprehensive analysis of the relationship between temperature, song features, and valence.

## Analysis of Variables {#sec-analysis}

### Outcome variable

The Valence variable exhibits a fairly uniform distribution, as shown in [@fig-valence], with values ranging from 0.036 to 0.981, as detailed in [@tbl-summary]. This wide range captures a broad spectrum of emotional tones in the dataset.

```{r}
#| label: fig-valence
#| fig-cap: Uniform Distribution of Valence
#| echo: false
#| warning: false
#| fig-align: center
#| fig.width: 5
#| fig.height: 3

ggplot(analysis_data, aes(x = Valence)) +
  geom_histogram(binwidth = 0.05, fill = "darkgreen", color = "black", alpha = 0.7) +
  labs(x = "Valence", y = "Count") +
  theme_minimal()
```

[@tbl-highest] highlights the Top 5 songs with the highest Valence, led by "September" by Earth, Wind & Fire (0.981), alongside other highly positive tracks like "Apple" by Charli XCX (0.962) and "Super Freak" by Rick James (0.962). These songs represent the most uplifting and positive moods in the dataset.

```{r}
#| label: tbl-highest
#| tbl-cap: Top 5 songs with the Highest valence
#| echo: false
#| warning: false
#| tbl-align: center
# Show the top 5 songs with the highest Valence
top_valence_songs_unique <- analysis_data %>%
  distinct(Song, Artist, Valence) %>%
  arrange(desc(Valence)) %>%
  head(5)

kable(top_valence_songs_unique, col.names = c("Song", "Artist", "Valence"))
```

In contrast, [@tbl-lowest] features the Top 5 songs with the lowest Valence, including "BLUE" by Billie Eilish (0.0365) and "Meteor Man" by Lil Uzi Vert (0.0384), which reflect darker or more somber tones. Together, these extremes illustrate the dataset's diversity, capturing a broad emotional spectrum across tracks.

```{r}
#| label: tbl-lowest
#| tbl-cap: Top 5 Songs with the Lowest Valence
#| echo: false
#| warning: false
#| tbl-align: center
# Show the top 5 unique songs with the lowest Valence
lowest_valence_songs_unique <- analysis_data %>%
  distinct(Song, Artist, Valence) %>% 
  arrange(Valence) %>%
  head(5)

kable(lowest_valence_songs_unique, col.names = c("Song", "Artist", "Valence"))
```

### Predictor variables

-   **Mean Temperature**

[@fig-temp] shows the average temperature over time, demonstrating a seasonal decline from approximately 25°C in the summer to below 5°C in late autumn. This pattern aligns with expected seasonal transitions in Canada and is further supported by [@tbl-summary], where the Mean Temperature averages 16.9°C with a standard deviation of 5.298°C. This significant variability provides an opportunity to investigate potential correlations between environmental factors, such as weather, and shifts in musical preferences reflected in Valence.
```{r}
#| label: fig-temp
#| fig-cap: Average Temperature from June to November 2024
#| echo: false
#| warning: false
#| fig-align: center
#| fig.width: 5
#| fig.height: 3

ggplot(analysis_data, aes(x = Date, y = `Mean Temp`)) +
  geom_line(color = "darkblue", alpha = 0.7) +
  labs(x = "Date", y = "Temperature (°C)") +
  theme_minimal()
```

\newpage
-   **Artist**

The variable Artist reveals noteworthy patterns in the dataset. Displayed in [@fig-artist], the top 10 artists, led by Sabrina Carpenter and Zach Bryan, contributed the highest number of entries to the dataset, with their songs appearing repeatedly in the daily Top 50 rankings, each exceeding 600 occurrences. This highlights their strong presence on the charts during the study period. However, despite their prominence, their songs do not feature in [@tbl-highest] or [@tbl-lowest]. In contrast, artists like Chappell Roan and Billie Eilish reflect a broader emotional range, appearing in the highest and lowest Valence categories, respectively. Hozier, contributing the fewest entries among the Top 10, stands out for appearing in [@tbl-highest], showcasing the emotional positivity of some of his music despite a smaller overall presence.
```{r}
#| label: fig-artist
#| fig-cap: Sabrina Carpenter and Zach Bryan leads the Top 10 artists
#| echo: false
#| warning: false
#| fig-align: center
#| fig.width: 5
#| fig.height: 3
# Get the top 10 artists by song count
top_artists <- analysis_data %>%
  count(Artist, sort = TRUE) %>%
  slice_max(n, n = 10) %>%
  mutate(Artist = gsub(" ", "\n", Artist))

ggplot(top_artists, aes(x = reorder(Artist, -n), y = n)) +
  geom_bar(stat = "identity", fill = "darkgreen", alpha = 0.7) +
  labs(x = "Artist",
       y = "Number of Tracks") +
  theme_minimal() +
  theme(axis.text.x = element_text(hjust = 0.5, size = 6))
```
\newpage
-   **Tempo**

The distribution of tempo values, as depicted in [@fig-tempo], shows a roughly normal shape, with most tracks falling between 100 and 130 beats per minute (BPM) and peaking around 120 BPM. This aligns with findings that today's popular music often falls within the 100–140 BPM range [@masterclass_bpm], emphasizing rhythmic patterns that resonate with mainstream listener preferences.

In addition, the distribution reveals outliers on both ends of the spectrum, with slower tempos below 60 BPM and faster tempos above 180 BPM representing niche genres or unique musical styles.

```{r}
#| label: fig-tempo
#| fig-cap: Normal Distribution of Tempo
#| echo: false
#| warning: false
#| fig-align: center
#| fig.width: 5
#| fig.height: 3

ggplot(analysis_data, aes(x = Tempo)) +
  geom_histogram(binwidth = 10, fill = "darkgreen", color = "black", alpha = 0.7) +
  labs(x = "Tempo (BPM)", y = "Frequency") +
  theme_minimal()
```
\newpage
-   **Danceability and Acousticness**

The [@fig-diff] below illustrates distinct patterns in the distributions of Danceability and Acousticness.

Acousticness displays a strongly right-skewed distribution, with most values concentrated near 0, demonstrating the prevalence of non-acoustic tracks in the dataset as noted in [@sec-overview]. This pattern aligns with its low mean of 0.228 and relatively high variability, as shown in [@tbl-summary], capturing a mix of purely electronic tracks and a smaller presence of acoustic elements.

In contrast, Danceability demonstrates a more uniform distribution, peaking between 0.6 and 0.8. With a higher mean of 0.632 and a standard deviation of 0.134, this indicates that the majority of tracks are moderately to highly danceable. This distribution reflects the dataset's focus on popular music, where rhythmic appeal and danceability are often emphasized.

```{r}
#| label: fig-diff
#| fig-cap: Acousticness highlights tracks with lower acoustic qualities, while Danceability shows a balanced distribution across rhythmic and energetic levels.
#| echo: false
#| warning: false
#| fig-align: center
#| fig.width: 6
#| fig.height: 4
#| fig.pos: "H" 
analysis_data %>%
  select(Danceability, Acousticness) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value") %>%
  ggplot(aes(x = Value, fill = Variable)) +
  geom_histogram(binwidth = 0.05, alpha = 0.7, position = "identity", color = "black") +
  facet_wrap(~Variable, scales = "free") +
  scale_fill_manual(values = c("Danceability" = "#4B0082", "Acousticness" = "#008080")) + # Custom colors
  labs(
    x = "Value",
    y = "Frequency"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```


### Exploring Relationships

-   **Valence vs. Temperature**

The heatmap in [@fig-heatmap] shows that higher temperatures (20-25°C) are linked to a greater frequency of tracks with moderately positive valence values, clustering around 0.5-0.75. In contrast, at lower temperatures (5-15°C), songs are distributed more evenly between positive and negative valence values. Interestingly, the highest frequency appears at a valence of approximately 0.25 when temperatures range between 15-20°C. The absence of data in the lowest temperature range (0-5°C) reflects a limitation of the dataset rather than an actual pattern.
```{r}
#| label: fig-heatmap
#| fig-cap: Higher temperatures align with positive valence values, while lower temperatures show a wider spread in emotional tones.
#| echo: false
#| warning: false
#| fig-align: center
#| fig.width: 5.5
#| fig.height: 3.5
#| fig.pos: "H" 
analysis_data %>%
  mutate(Temp_Range = cut(`Mean Temp`, breaks = seq(0, 25, by = 5))) %>%
  ggplot(aes(x = Temp_Range, y = Valence, fill = after_stat(count))) + 
  stat_bin2d(binwidth = c(1, 0.1)) +
  scale_fill_gradient(low = "lightblue", high = "darkblue", name = "Count") +
  labs(x = "Temperature Range (°C)",
    y = "Valence (Happiness)"
  ) +
  theme_minimal() +
  annotate(
    "text", x = 1, y = 0.05, label = "No data in this region", color = "darkred", size = 2.5
  ) +
  theme (
    legend.key.size = unit(0.4, "cm"), 
    legend.text = element_text(size = 7), 
    legend.title = element_text(size = 8) 
  )
```

-   **Valence vs. Artist**

The chart in [@fig-avgvalence] shows distinct contrasts in the emotional tone of music produced by the Top 10 artists. Hozier, Tommy Richman, and Chappell Roan stand out with the highest average Valence scores, indicating their tracks lean toward more positive and uplifting moods. This pattern aligns with their reputations for creating emotionally evocative music. On the other hand, Kendrick Lamar and Zach Bryan, who rank lowest in average Valence, reflect a focus on deeper or more introspective themes, consistent with their storytelling-driven styles.
\newpage
```{r}
#| label: fig-avgvalence
#| fig-cap: Hozier leads with the most positive emotional tone, while Kendrick Lamar features the lowest average valence among the top contributors.
#| echo: false
#| warning: false
#| fig-align: center
#| fig.width: 5.5
#| fig.height: 3.5
#| fig.pos: "H" 
# Average Valence by Artist (Top 10 Artists)
top_artists <- analysis_data %>%
  group_by(Artist) %>%
  summarize(avg_valence = mean(Valence), count = n()) %>%
  arrange(desc(count)) %>%
  slice(1:10) %>%
  mutate(Artist = gsub(" ", "\n", Artist))

ggplot(top_artists, aes(x = reorder(Artist, avg_valence), y = avg_valence, fill = avg_valence)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(
    x = "Artist",
    y = "Average Valence"
  ) +
  theme_minimal() +
  theme(axis.text.y = element_text(hjust = 0.5, size = 6))
```

# Model {#sec-model}

This section presents a multiple linear regression (MLR) model to predict song valence based on audio features and contextual factors. It examines how danceability, acousticness, scaled tempo, scaled mean temperature, and artist-specific effects influence valence, providing a structured approach for analyzing these relationships.

The analysis utilized the R packages, stats [@citeR], MLmetrics [@MLmetrics], and car [@car]. The stats package supported MLR modeling with the `lm` function, while MLmetrics calculated RMSE and R-squared for model evaluation. The car package tested for multicollinearity using Variance Inflation Factors (VIF).

## Model Structure

Mathematical Representation:

\begin{align}
y_i &= \beta_0 + \beta_1 (\text{Scaled Mean Temp})_i + \beta_2 (\text{Artist})_i + \beta_3 (\text{Danceability})_i \\
&\quad + \beta_4 (\text{Acousticness})_i + \beta_5 (\text{Scaled Tempo})_i + \epsilon_i
\end{align}

-   $y_i$: Valence of the song, measuring perceived positivity.
-   $\beta_0$: Baseline valence when predictors are at their reference levels.
-   $\beta_1$: Effect of scaled mean temperature on valence.
-   $\beta_2$: Artist-specific effects on valence.
-   $\beta_3$: Effect of danceability on valence.
-   $\beta_4$: Effect of acousticness on valence.
-   $\beta_5$: Effect of scaled tempo on valence.
-   $\epsilon_i$: Unexplained variability, assumed to follow a normal distribution with mean 0 and variance.

## Variable Selection {#sec-variable}

In constructing the model, careful consideration was given to selecting predictors that accurately capture the relationships influencing song valence:

-   *Scaled Mean Temperature*: Represents environmental effects on emotional responses to music. Scaling normalizes its range, aiding interpretability and comparability with other predictors.

-   *Artist*: Accounts for individual stylistic influences, as certain artists consistently produce music associated with higher or lower valence.

-   *Danceability*: Captures rhythm and energy, with higher values often linked to more positive emotions, thereby increasing valence.

-   *Acousticness*: Reflects the degree of acoustic elements in a track. Higher acousticness aligns with calmer tones that may lower valence, while lower acousticness corresponds to dynamic, upbeat tracks.

-   *Scaled Tempo*: Measures song speed, influencing energy and emotional tone. Faster tempos are often associated with higher valence. Scaling ensures consistency with other predictors, like Scaled Mean Temperature.

## Model Validation 

The model validation process involved out-of-sample testing, where the dataset was split into training and testing sets in a 70-30 ratio. Three variations of the multiple linear regression model were tested, differing in the number of predictors included. Model performance was evaluated using Root Mean Square Error (RMSE) and R-squared values. As shown in [@tbl-comp], the models demonstrate varying degrees of predictive accuracy and explanatory power.

```{r}
#| label: tbl-comp
#| tbl-cap: Comparison of Model Performance
#| echo: false
#| warning: false
#| tbl-align: center

comparison_data <- data.frame(
  Model = c("MLR 1", "MLR 2", "MLR 3"),
  Variables = c(
    "Scaled Mean Temp, Artist, Danceability, Acousticness, Scaled Tempo, Date",
    "Scaled Mean Temp, Artist, Danceability, Acousticness, Scaled Tempo",
    "Scaled Mean Temp, Danceability, Acousticness, Scaled Tempo"
  ),
  RMSE = c(0.0868, 0.0866, 0.2031),
  `R-Squared` = c(0.8597, 0.8601, -1.4267),
  check.names = FALSE
)

kable(
  comparison_data,
  format = "pipe",
  align = "l",
  col.names = c("Model", "Variables", "R-Squared", "RSME")
) |> 
  kable_styling(
    full_width = FALSE, 
    position = "center", 
    font_size = 10
  ) |> 
  column_spec(1, width = "3cm") |>  
  column_spec(2, width = "7cm") |> 
  column_spec(3:4, width = "2cm") 

```

After not taking into consideration Artist, MLR 3 includes fewer predictors, it performs significantly worse, as evidenced by its negative R-squared and much higher RMSE, indicating poor fit and predictive ability. On the other hand, MLR 2 demonstrates a slight performance advantage over MLR 1 in both RMSE and R-squared metrics.

## Model Justification

MLR 2 was selected for its balance of predictive accuracy and simplicity. The `Date` variable was excluded as its relationship with `Scaled Mean Temp` made it redundant, avoiding unnecessary complexity. By focusing on the key predictors outlined in @sec-variable, the model effectively addresses the research objectives. Additionally, MLR 2 demonstrated better RMSE and R-squared values compared to MLR 1, further supporting its choice as the final model for predicting song valence.

## Model Diagnostics

To demonstrate why this model is optimal, the following tests and evaluations were conducted to assess its performance and reliability. A plot comparing the predicted and true valence values was generated to visually assess the model's performance, [@fig-predicted]. The dashed line represents the 1:1 relationship, indicating perfect predictions. Most points cluster around the line, showing that the model effectively captures the relationship between predictors and valence. However, some deviations are observed, which may indicate areas where the model could be improved.

The Variance Inflation Factor values, shown in [@tbl-vif], evaluate potential multicollinearity among predictors. Although `Artist` has a high raw VIF of 61.91 due to its many levels, the adjusted value of 1.01 in the last column accounts for its degrees of freedom, confirming that multicollinearity is not a concern for this categorical variable. Other predictors—`Scaled Mean Temp`, `Danceability`, `Acousticness`, and `Scaled Tempo`—have VIF values below 6, further indicating no significant multicollinearity in the model.

Additionally, residual plots and Q-Q plots were generated as part of the diagnostics to evaluate model assumptions. A more detailed analysis of these plots and their implications is provided in @sec-discussion.

```{r}
#| label: tbl-vif
#| tbl-cap: Variance Inflation Factor for Model Predictors
#| echo: false
#| warning: false
#| tbl-align: center
vif_values <- vif(mlm_model)

vif_table <- as.data.frame(round(vif_values, 2)) %>%
  rownames_to_column(var = "Predictor")

kable(vif_table, format = "pipe", col.names = c("Predictor", "GVIF", "Df", "GVIF^(1/(2*Df))"))
```

# Results {#sec-results}

```{r}
#| echo: false
#| eval: true
#| warning: false
mlm_data_analysis <- analysis_data %>%
  mutate(predicted_valence_lm = predict(mlm_model, newdata = analysis_data))
```

The multiple linear regression (MLR) analysis identifies significant predictors of Valence, detailed in [@tbl-modelresults] and visualized in [@fig-coef]. These findings build upon the descriptive patterns of the outcome and predictor variables, detailed in [@sec-analysis].

Danceability is identified as the strongest predictor, with a substantial positive coefficient 1.188 and a t-statistic of 75.469, demonstrating its strong and statistically significant association with higher Valence. Its adjusted R\^2 value of 0.081 indicates that this variable alone explains 8.1% of the variability in Valence, highlighting the central role of rhythmic and energetic qualities in shaping positive emotional responses to music. In [@fig-diff], the distribution of Danceability values, peaking at moderate to high levels, further underscores its prominence in popular tracks.

Scaled Tempo demonstrates a statistically significant positive effect on Valence, with a coefficient of 0.054 and a precise estimate supported by a small standard error of 0.002. As illustrated in [@fig-tempo], the original distribution of Tempo, approximately normal with a peak around 120 BPM, provides important context for interpreting this relationship. Scaling centers and standardizes the variable without altering its underlying patterns or associations. According to [@tbl-contribution], Scaled Tempo explains 1.8% of the variability in Valence, highlighting its subtle yet meaningful role in influencing musical positivity.
```{r}
#| echo: false
#| eval: true
#| label: tbl-modelresults
#| tbl-cap: >
#|   The modeling originally includes all artists, but for readability, only the top 5 artists with the
#|   highest positive and negative impact on valence are presented here, alongside all other variables of interest.
#| warning: false

# Create a data frame with the manually formatted data
formatted_table <- tibble::tibble(
  Term = c(
    "(Intercept)", 
    "Scaled Mean Temp", 
    "Andy Williams", 
    "Bobby \"Boris\" Pickett, The Crypt-Kickers",
    "Brenda Lee", 
    "Dean Martin", 
    "Eminem, JID", 
    "Future", 
    "Future, Metro Boomin, Kendrick Lamar", 
    "Gunna", 
    "Kendrick Lamar", 
    "Van Morrison", 
    "Danceability", 
    "Acousticness", 
    "Scaled Tempo"
  ),
  Estimate = c(
    0.059, 0.003, 0.407, 0.210, 0.381, 
    0.331, -0.986, -0.776, -0.778, 
    -0.820, -0.868, 0.268, 1.188, -0.259, 0.054
  ),
  `Std. Error` = c(
    0.017, 0.001, 0.085, 0.044, 0.033, 
    0.085, 0.033, 0.037, 0.018, 
    0.019, 0.015, 0.019, 0.016, 0.008, 0.002
  ),
  Statistic = c(
    3.513, 2.473, 4.793, 4.731, 11.418, 
    3.892, -30.309, -21.177, -43.222, 
    -42.503, -56.843, 14.023, 75.469, -30.727, 35.226
  )
)
  
kable(formatted_table, format='pipe')
```

Scaled Mean Temperature exhibits the smallest yet statistically significant positive coefficient (0.003, t = 2.473). Its seasonal variability, transitioning from 25°C to below 5°C as summarized in [@tbl-summary], reflects a subtle relationship with Valence. This aligns with the prior study referenced in @sec-intro of @anglada2023music, which found that higher temperatures tend to be associated with music featuring happier tones. However, the impact observed in this study is smaller than anticipated, consistent with its modest adjusted R\^2 value of 0.0007 in [@tbl-contribution]. This finding suggests that while temperature does play a role in shaping musical perception, its effect is considerably less pronounced compared to key audio features like Danceability and Tempo.

In contrast, Acousticness exhibits a significant negative effect on the outcome variable, with a coefficient of -0.259, accounting for approximately 1.3% of its variability. This suggests that tracks with pronounced acoustic features, often associated with introspective or somber tones, are perceived as less positive. This finding aligns with the nature of acoustic music, which typically evokes calmer and more reflective emotions, contrasting with the dynamic and energetic qualities linked to higher levels of Valence.

```{r}
#| echo: false
#| eval: true
#| label: tbl-contribution
#| tbl-cap: Predictors' Adjusted R^2 Contributions
#| warning: false
# Full model adjusted R-squared
full_r2 <- summary(mlm_model)$adj.r.squared

# Calculate adjusted R-squared after removing each variable
model_no_artist <- lm(Valence ~ Acousticness + `Scaled Mean Temp` + `Scaled Tempo` + Danceability, data = analysis_data)
artist_contribution <- full_r2 - summary(model_no_artist)$adj.r.squared

model_no_acousticness <- lm(Valence ~ Artist + `Scaled Mean Temp` + `Scaled Tempo` + Danceability, data = analysis_data)
acousticness_contribution <- full_r2 - summary(model_no_acousticness)$adj.r.squared

model_no_temp <- lm(Valence ~ Artist + Acousticness + `Scaled Tempo` + Danceability, data = analysis_data)
temp_contribution <- full_r2 - summary(model_no_temp)$adj.r.squared

model_no_tempo <- lm(Valence ~ Artist + Acousticness + `Scaled Mean Temp` + Danceability, data = analysis_data)
tempo_contribution <- full_r2 - summary(model_no_tempo)$adj.r.squared

model_no_danceability <- lm(Valence ~ Artist + Acousticness + `Scaled Mean Temp` + `Scaled Tempo`, data = analysis_data)
danceability_contribution <- full_r2 - summary(model_no_danceability)$adj.r.squared

contributions <- data.frame(
  Variable = c("Artist", "Acousticness", "Scaled Mean Temp", "Scaled Tempo", "Danceability"),
  Adjusted_R2_Contribution = c(
    artist_contribution,
    acousticness_contribution,
    temp_contribution,
    tempo_contribution,
    danceability_contribution
  )
)
kable(contributions, col.names = c("Variable", "Adjusted R^2"))
```

The categorical variable Artist accounts for the largest portion of Valence variability, with an adjusted R\^2 value of 0.589 in [@tbl-contribution]. This reflects the diverse and unique qualities that different artists bring to their music, including stylistic variations, genre-specific elements, emotional themes, and even their popularity. Unlike continuous predictors (e.g., Acousticness or Tempo), Artist captures a range of unmeasured attributes—such as vocal style, lyrical content, and production techniques—that significantly shape listener perception.

The variable’s numerous levels, representing individual artists, further enhance its ability to explain variance in the emotional tone of songs. Each artist’s unique influence contributes significantly to the model’s complexity, resulting in substantially higher predictive accuracy compared to other predictors.

@fig-coef visually complements the statistical findings by summarizing the significance, direction, and magnitude of the predictors. Each point represents a predictor’s estimated coefficient, with horizontal lines indicating the 95% confidence intervals. Predictors with larger t-statistics, reflected by narrower confidence intervals, demonstrate greater reliability in their estimated effects.

While both artists exhibit strong positive effects, Brenda Lee's narrower confidence interval indicates greater reliability compared to Andy Williams, despite his slightly higher coefficient. Conversely, Kendrick Lamar demonstrates a pronounced negative association, reflecting contrasting musical styles and emotional themes. The figure further emphasizes Danceability as the most impactful predictor while highlighting the nuanced contributions of Scaled Tempo, Scaled Mean Temperature, and artist-level effects, further validating the robustness of the MLR model.

```{r}
#| echo: false
#| eval: true
#| label: fig-coef
#| fig-cap: Impact of Key Features on Emotional Tone in Music
#| warning: false
#| fig-align: center
#| fig.width: 5.5
#| fig.height: 3.5
#| fig.pos: "H"

# Extract coefficients from the model
tidy_model <- broom::tidy(mlm_model)

# Separate artist terms and clean the names
artist_coefficients <- tidy_model %>%
  filter(str_detect(term, "Artist")) %>%
  mutate(term = str_replace(term, "Artist", "")) %>%  # Remove "Artist"
  arrange(desc(abs(estimate)))

# Select only top 5 positive and negative artists
top_positive <- artist_coefficients %>%
  slice_max(order_by = estimate, n = 5)

top_negative <- artist_coefficients %>%
  slice_min(order_by = estimate, n = 5)

# Combine top positive and negative artists
impactful_artists <- bind_rows(top_positive, top_negative)

# Include non-artist predictors (scaled tempo, acousticness, etc.)
non_artist_coefficients <- tidy_model %>%
  filter(!str_detect(term, "Artist"))

# Combine impactful artists and non-artist predictors
filtered_model <- bind_rows(non_artist_coefficients, impactful_artists)

# Plot the filtered coefficients
ggplot(filtered_model, aes(x = fct_reorder(term, estimate), y = estimate)) +
  geom_point(color = "blue", size = 3) +  # Points for coefficients
  geom_errorbar(
    aes(ymin = estimate - 1.96 * std.error, ymax = estimate + 1.96 * std.error), 
    width = 0.2, color = "gray"
  ) +  # Error bars
  coord_flip() +  # Flip coordinates for readability
  labs(
    x = NULL,  
    y = "Coefficient Estimate"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    axis.text.y = element_text(size = 5, color = "gray20"),  # Adjust font size
    axis.text.x = element_text(size = 6, color = "gray20"),
    axis.title = element_text(color = "gray30")
  )
```

# Discussion {#sec-discussion}

This study applied a multiple linear regression (MLR) framework to examine the relationships between song characteristics, temperature, and valence, a metric representing emotional positivity in music. By integrating Spotify’s audio feature data with population-weighted mean temperature, the analysis quantified the contributions of key predictors, including danceability, acousticness, tempo, and artist-specific effects, to valence. Statistically significant effects were identified for most predictors, while redundant variables like date were excluded to ensure model simplicity. The chosen model demonstrated a predictive power of 86% (R-squared), effectively capturing the observed relationships while leaving room for further refinement.

The findings illustrate how specific audio features and contextual factors shape listeners' emotional perceptions of music. Danceability emerged as the most influential predictor, highlighting the strong association of rhythmic and energetic qualities with perceptions of positivity. Tempo and acousticness also exhibited statistically significant effects: faster tempos aligned with more positive emotional tones, while acoustic tracks correlated with calmer or introspective moods. These results enhance our understanding of how musical attributes evoke emotional responses.

Beyond intrinsic features, contextual factors such as mean temperature subtly influenced listeners’ musical experiences. Although its contribution was modest compared to predictors like danceability, the significant relationship between temperature and valence suggests that environmental contexts influence musical perception. This opens pathways for further exploration of how external factors like weather and cultural background shape musical preferences and emotional engagement.

## Weaknesses of the Approach

Despite the robustness of the MLR framework, several limitations remain. In @fig-assumptions, Residual and Q-Q plots revealed deviations from normality and heteroscedasticity, indicating that the model may not fully capture nonlinearities or interactions among variables. While the results are interpretable within the current framework, adopting alternative modeling techniques, such as generalized additive models, could better account for these complexities and improve predictive accuracy.

The use of population-weighted mean temperature, though reflective of major population centers, overlooks localized environmental variations. This limitation reduces the precision of the temperature variable in capturing the diverse conditions experienced across Canada. Incorporating city-specific or real-time weather data could yield a more nuanced understanding of temperature's relationship with valence.

The focus on Top 50 songs excludes less prominent artists, limiting the generalizability of the findings. Lesser-known artists may introduce unique stylistic or emotional elements absent from the dataset. Expanding the scope to include a broader range of artists would provide a more representative analysis.

Finally, the six-month study period constrains the ability to explore seasonal or long-term trends. While this timeframe offers a snapshot of current patterns, extending the dataset to cover multiple years would enable a deeper examination of how musical preferences shift over time and identify potential seasonal effects.

These limitations underscore the need for future research to refine and expand this approach by adopting more flexible models, incorporating broader datasets, and considering more granular contextual information to better understand the relationship between music, context, and emotion.

## Future Directions and Next Steps

To address the identified limitations, future research could incorporate more granular contextual data, such as city-level weather patterns or regional cultural influences, to better capture localized environmental factors shaping listeners’ emotional responses to music. Expanding the dataset to examine variations across different religions, cultural backgrounds, or linguistic groups within Canada would further enhance the findings’ generalizability, reflecting the country’s rich stylistic and cultural diversity.

Incorporating temporal trends presents another promising direction. Extending the dataset to span multiple years would allow for the investigation of seasonal patterns and long-term shifts in musical preferences, similar to the approach by @anglada2023music. Adding variables such as lyrical content and genre could further enrich the analysis by providing insights into how lyrical themes and musical styles interact with listener sentiment, fostering a deeper understanding of the interplay between song characteristics and emotional perception.

Finally, adopting advanced modeling techniques, such as machine learning or Bayesian frameworks, could significantly improve predictive accuracy and reveal complex, nonlinear relationships among variables. These approaches would build upon the current findings, enabling a more sophisticated exploration of the multifaceted influences of music, context, and emotion while addressing the limitations of traditional regression-based models.

\newpage

\appendix

# Appendix {.unnumbered}

# Survey, Sampling & Observational Data

## Spotify Data Collection and Handling Process

Spotify collects streaming data from its extensive user base to reflect music consumption patterns, as detailed in [@spotify_art]. A stream is recorded when a listener plays a song or music video for at least 30 seconds, and streams from downloaded content are included only if the listener goes online at least once every 30 days. These streams contribute to metrics such as all-time streams, release stream counts, and individual track plays. However, not all streams are chart-eligible; Spotify applies a proprietary formula to ensure that the charts accurately represent legitimate user-driven activity.

**Data Validation and Fraud Prevention**

Spotify uses machine learning algorithms to identify and exclude streams suspected of being generated fraudulently, such as those created by bots or through artificial manipulation. This process ensures that charts are not distorted and truly reflect genuine user activity. Importantly, this filtering does not affect the royalties paid to artists, maintaining financial transparency.

**Types of Charts**

Spotify offers several chart types, updated daily and weekly, to capture different aspects of music trends:

-   Global and Regional Charts: Reflect overall listening patterns across regions using aggregated, eligible streams.
-   City Charts: Focus on popular tracks within specific cities, highlighting localized listening behaviors.
-   Viral Charts: Rank songs gaining social media traction, based on rising play counts, sharing activity, and new listener discoveries.

Daily charts are published by 6 PM EST the day after the charting period, while weekly charts are released globally after the week ends on Thursday. Chart eligibility includes any song or album live on Spotify during the respective time period.

**Handling Missing or Anomalous Data**

Although Spotify does not disclose specific methodologies for addressing missing or incomplete data, it maintains reliability by:

-   Filtering irregular streams using machine learning algorithms.
-   Applying consistent validation thresholds, such as the 30-second playback rule.
-   Regular updates that minimize potential reporting gaps.

**Strengths**

-   Detailed Analysis: Spotify provides data at global, regional, and city levels, enabling refined examination of user behavior.
-   Fraud Detection: Machine learning algorithms effectively exclude non-genuine streams, preserving the credibility of its charts.
-   Broad Metrics: Including all-time streams, release-specific counts, and daily/weekly charts ensures a thorough overview of music consumption patterns.

**Weaknesses**

-   Active User Bias: Charts primarily reflect preferences of highly active users, potentially underrepresenting less frequent listeners or those engaging with niche genres.
-   Exclusion of Non-charting Songs: The analysis focuses on a limited selection of songs included in Spotify’s charts, such as the Top 100 for daily charts or the Top 50 for other charts, which restricts insights into underrepresented musical tastes and niche genres that do not appear in these rankings.
-   Opaque Methodologies: Spotify's proprietary formula and data-cleaning processes are not publicly disclosed, hindering external validation.

This structured approach allows Spotify to capture significant patterns in music consumption while addressing challenges such as fraudulent activity and regional representation. However, improvements in methodological transparency and broader inclusion of non-charting songs could further expand its analytical utility.

## Weather Data Collection and Handling Process

Environment and Climate Change Canada (ECCC), as detailed in @ECCC_technical_documentation, collects extensive weather and climate data through an extensive network of observation stations, satellite systems, and historical archives. These datasets form the foundation for climate analysis, weather forecasting, and research on environmental impacts, ensuring a thorough understanding Canada’s diverse climatic conditions.

**Data Sources and Methodology**

1.  **Observation Stations**
    -   Over 600 weather stations across Canada adhere to World Meteorological Organization (WMO) standards, ensuring global comparability.
    -   These stations record key atmospheric parameters, such as temperature, precipitation, and wind speed, at hourly, daily, and monthly intervals.
    -   Advanced automated systems, like the Lansdowne House station, feature specialized instruments, including tipping bucket gauges (rainfall), snow depth sensors, and Stevenson screens (temperature and humidity).
2.  **Satellite and Remote Sensors**
    -   Satellites and weather balloons provide supplementary data, particularly in remote regions, capturing atmospheric profiles and surface observations.
3.  **Historical Records**
    -   Canada’s climatological record dates back to 1840, with daily climate data available since 1870 and hourly observations since 1953.
4.  **Gridded and Point Data**
    -   Data is available as point data (individual stations) or gridded datasets, where spatial interpolation estimates conditions between stations.

**Data Validation and Handling**

1.  **Quality Control**

-   ECCC applies automated checks to identify anomalies or missing values, flagged with markers (e.g., “-99999” for missing data).
-   Manual reviews correct biases introduced by factors such as station relocations, instrument changes, or urbanization effects.

2.  **Missing and Anomalous Data**

-   Missing values are flagged and interpolated using spatial techniques where appropriate.
-   Metadata documents causes of data gaps, such as operational disruptions or equipment malfunctions.

3.  **Spatial Interpolation**

-   Non-station observations are estimated using gridded datasets, which incorporate data from nearby stations through spatial modeling techniques.

4.  **Data Formats**

-   Weather records are stored in various formats (e.g., hourly \[HLY01\], daily \[DLY02, DLY04\], monthly \[MLY04\]) with detailed metadata for traceability.

**Strengths**

-   Wide-ranging Coverage: Regularly updated data from a dense network of stations provides high temporal and spatial resolution.
-   Rigorous Quality Control: Automated and manual quality control ensures reliability, even for long-term climate studies.
-   Accessibility: Data availability in multiple formats (point and gridded) supports diverse research and policy applications.
-   Transparency and Metadata: Extensive documentation enhances usability and fosters transparency.

**Weaknesses**

-   Urbanization Effects: Changes in urban landscapes around stations can introduce biases in long-term temperature or precipitation records, requiring careful adjustments.

-   Sparse Coverage in Remote Areas: Northern Canada’s limited station density reduces granularity despite interpolation efforts.

-   Complex Formatting: Intricate flagging systems may pose challenges for non-expert users.

-   Reliance on Adjustments: Historical comparability relies on accurate corrections for station relocations, instrument upgrades, and methodological changes.

**Linkages to Literature and Methodological Insights**

-   The use of 30-year reference periods aligns with WMO standards, ensuring international comparability and consistency in climate analyses.

-   Spatial interpolation techniques, widely supported in climate research, enhance the usability of datasets for environmental modeling and forecasting.

-   Peer-reviewed studies emphasize the importance of adjustments for urbanization and station relocations in preserving the integrity of long-term climate records.

This framework underscores the importance of methodological rigor, ongoing refinement, and technological advancements to ensure the accuracy and relevance of climate data while addressing associated challenges.

## Conclusion and Future Directions

This section emphasizes the structured approach and challenges associated with integrating observational data from diverse sources. By addressing these challenges and adopted enhanced data validation techniques, the study lays the groundwork for future research exploring the intersection of music, environment, and emotion. Future studies could enhance representativeness by incorporating additional data sources, such as user surveys or alternative streaming platforms, to validate and expand upon these findings.

# Idealized Survey & Methodology - \$100K Budget

The aim of this research is to investigate the interplay between weather conditions and music consumption preferences across Canadian cities, leveraging Spotify's streaming data and Environment and Climate Change Canada's (ECCC) weather datasets. With a \$100,000 budget, the hypothetical survey outlined below is designed to address some of the key limitations identified in the current research methodology, such as underrepresentation of smaller cities, lack of granular cultural insights, and temporal patterns in music preferences.

## Sampling Approach

**Target Population**

-   Primary: Active Spotify users in Canada, aged 18–65, residing in all Canadian cities with ECCC weather stations.
-   Secondary: Broader music listeners engaging with other platforms, to ensure diversity and comparability.

**Sampling Strategy**

1.  Stratified Random Sampling:

-   City: All cities with weather stations to capture diverse climatic and cultural conditions.
-   Age Groups: 18–24, 25–34, 35–44, 45–54, 55–65.
-   Gender: Male, Female, Non-binary.
-   Cultural and Linguistic Diversity: Representation of English, French, Indigenous, and multilingual speakers.

2.  Sample Size: 3,000 participants, proportionally distributed across cities based on population size. This balance ensures representation from smaller cities, addressing their underrepresentation in previous datasets.

3.  Data Collection Period: 12 months, capturing seasonal variations and long-term trends in weather and music preferences.

4.  Temporal Trends and Contextual Variables: Incorporate temporal patterns by comparing seasonal shifts across cities and genres, enriching the dataset with cultural events and holidays.

## Survey Implementation

**Recruitment Strategy**

1.  Online Recruitment:

-   Use Spotify ads, Google Ads, and social media platforms tailored to regional audiences.
-   Partner with local organizations to boost participation in smaller cities.
-   Budget: \$30,000.

2.  Incentives:

-   Participants receive \$10 gift cards, with targeted outreach to underrepresented demographics.
-   Budget: \$30,000.

3.  Inclusive Recruitment:

-   Outreach campaigns in Indigenous communities and multilingual regions to enhance representation.

**Survey Design** 

Questions structured to elicit insights on music consumption, emotional responses, and cultural factors:

-   How does weather affect your choice of music (e.g., calm, energetic, reflective)?
-   Do you prefer certain genres or lyrics during specific seasons or weather conditions?
-   How do cultural or linguistic backgrounds shape your music preferences?
-   How does time of day (morning, afternoon, night) impact your music choices?
-   What external factors (e.g., holidays, work environment, commuting) influence your listening habits?

## Observational Data Collection

**Integration with ECCC Weather Data**

-   Hourly Weather Metrics: Link survey responses to city-level hourly weather data, ie. temperature, precipitation, snow depth, humidity, and wind speed.
-   Cultural Events & Holidays: Cross-reference with music trends during regional holidays and events.
-   Air Quality Index (AQI): Analyze how air quality influences mood and listening preferences.
-   City-Level Data: Match survey responses to Spotify’s city-level charts, including valence, energy, danceability, and genre diversity.
-   Temporal Trends: Examine changes in listening patterns by day, week, and season.

## Addressing Limitations

This survey design tackles key limitations by broadening its geographic and demographic representation. By including all Canadian cities with weather stations, it ensures coverage beyond the usual focus on six major urban centers, offering a more thorough understanding of city-level variations in music preferences. Moreover, stratifying the sample by linguistic and cultural groups mitigates biases toward dominant cultural narratives, capturing a broader and detailed picture of regional and cultural influences on music consumption.

Additionally, the year-long data collection period enables the analysis of seasonal and temporal trends, uncovering how weather impacts music consumption across different times of the year. The inclusion of granular contextual variables, such as cultural events, air quality, and commuting behavior, strengthens the analysis by addressing non-climatic factors that shape music preferences. This approach provides a comprehensive view of the interplay between weather and music consumption patterns.

## Data Validation and Modeling

To ensure data quality, the survey incorporates robust validation techniques. Attention checks, such as instructing respondents to select a specific answer, help identify inattentive responses. Logic checks are employed to catch inconsistencies, such as self-reported outdoor listening during severe weather conditions. Additionally, post-stratification weighting adjusts for demographic imbalances, ensuring the sample accurately reflects Canada’s population distribution and increases the reliability of the findings.

Advanced modeling approaches further refine the analysis by addressing complex relationships within the data. Bayesian Hierarchical Models capture city-level variability and nonlinear interactions, providing detailed insights into regional differences. Machine learning models are also leveraged to predict how external factors like weather influence music preferences and emotional responses, enabling a deeper understanding of the interplay between climate and music consumption.

## Budget Breakdown

```{r}
#| echo: false
#| eval: true
#| warning: false
budget_allocation <- data.frame(
  `Expense Category` = c("Online Recruitment", 
                       "Incentives for Participants", 
                       "Data Processing & Validation", 
                       "Survey Software (e.g., Qualtrics)", 
                       "Miscellaneous Costs"),
  `Budget Allocation` = c(30000, 30000, 20000, 10000, 10000),
  check.names = FALSE
)

kable(budget_allocation, format='pipe')
```

## Conclusion

With a \$100,000 budget, this survey framework leverages stratified sampling, city-level weather data, and advanced modeling techniques to address current research limitations. By incorporating broader geographic representation, cultural diversity, and temporal trends, the proposed methodology enhances the understanding of how external factors shape music preferences, providing a richer and more credible foundation for future studies.

# Supporting Information


## Weather Station Selection and Mean Temperature Calculation {#sec-appendix-a}

To ensure the weather data accurately reflects conditions across Canada's major urban centers, a population-weighted sampling strategy was employed. This approach incorporates temperature data from six key cities: Toronto, Montreal, Vancouver, Calgary, Edmonton, and Ottawa. These cities were chosen based on their significant populations and their role as representative urban hubs in different regions of the country.

Daily temperature readings were sourced from the nearest reliable weather stations to each city: Toronto City for Toronto, McTavish for Montreal, Vancouver Harbour CS for Vancouver, Calgary International CS for Calgary, Edmonton Blatchford for Edmonton, and Ottawa CDA RCS for Ottawa. These stations were selected for their proximity to the city centers and their consistent, high-quality temperature data.

The mean temperature was calculated using a population-weighted average, ensuring that each city’s contribution to the overall calculation was proportional to its population. The formula used was: 

\begin{align}
\text{Mean Temperature} = \frac{\sum (\text{City Population} \times \text{Temperature})}{\sum \text{City Population}}
\end{align}


@tbl-prox provides details on the population and weather station used for each city in the calculation:

```{r}
#| echo: false
#| eval: true
#| label: tbl-prox
#| tbl-cap: Population and Weather Stations for Major Canadian Cities
#| warning: false
# Create the data frame
city_data <- data.frame(
  City = c("Toronto", "Montreal", "Vancouver", "Calgary", "Edmonton", "Ottawa"),
  Station = c("Toronto City", "McTavish", "Vancouver Harbour CS", 
              "Calgary Int'l CS", "Edmonton Blatchford", "Ottawa CDA RCS"),
  Population = c(6200000, 4200000, 2700000, 1600000, 1500000, 1400000)
)

city_data %>%
  kable(col.names = c("City", "Station", "Population")) %>%
  kable_styling(full_width = FALSE, position = "center") %>%
  row_spec(0, align = "c")
```

This method emphasizes urban areas, capturing the majority of Canada’s population while balancing representativeness with practical feasibility. However, it may not fully account for weather variations in smaller cities or rural regions.

By prioritizing proximity and reliable data collection, this strategy ensures an accurate and regionally representative calculation of average weather conditions.

## Summary of Related Research

The study Here Comes the Sun: Music Features of Popular Songs Reflect Prevailing Weather Conditions by @anglada2023music analyzed the relationship between weather conditions and audio features of songs that appeared in the United Kingdom's weekly top charts from 1953 to 2019, comprising 23,859 unique entries. The researchers examined three key weather variables: daily maximum temperature, hours of sunshine, and days with rainfall, alongside audio features such as energy, danceability, and valence.

The study found that high-arousal, positive music (e.g., energetic, danceable, and emotionally uplifting songs) was positively associated with higher temperatures and sunshine hours but negatively correlated with rainfall. These relationships were strongest during colder months when changes in weather conditions were most noticeable. By contrast, low-arousal, negative music (e.g., calm or melancholic songs) showed no significant relationship with weather. This research highlights how weather conditions can influence population-level preferences for music, particularly for songs with features reflecting positivity and activity.

## Model's Assumptions

```{r}
#| echo: false
#| eval: true
#| label: fig-assumptions
#| fig-cap: Residuals vs Fitted and Normal Q-Q Plot
#| warning: false
#| fig-align: center
#| fig.width: 5.5
#| fig.height: 3.5
# Arrange plots side by side
par(mfrow = c(1, 2))

# Residual vs. Fitted Plot
plot(mlm_model, which = 1)

# Normal Q-Q Plot
plot(mlm_model, which = 2)
```

```{r}
#| echo: false
#| eval: true
#| label: fig-predicted
#| fig-cap: Comparison of Predicted and True Valence
#| warning: false
#| fig-align: center
#| fig.width: 5.5
#| fig.height: 3.5
ggplot(mlm_data_analysis, aes(x = Valence, y = predicted_valence_lm)) +
  geom_point(alpha = 0.5, color = "orange", size = 2) + 
  geom_abline(intercept = 0, slope = 1, color = "darkblue", linetype = "longdash", linewidth = 1) +
  labs(
    x = "True Valence",  
    y = "Predicted Valence" 
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    axis.text = element_text(color = "gray20"),
    axis.title = element_text(color = "gray30")
  )
```

\newpage

# References
